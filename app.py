# -*- coding: utf-8 -*-
"""clickbait detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CGVSEfn5Z3DIt_OPACjn-yiQht4tYOB
"""

import streamlit as st
import pandas as pd
import numpy as np
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow import squeeze
from transformers import XLNetTokenizer, TFXLNetModel
from sklearn.model_selection import train_test_split

DATA_PATH = 'https://raw.githubusercontent.com/Benrir/NLP/main/train1.csv'
data = pd.read_csv(DATA_PATH, encoding='utf-8', delimiter=';')
st.write(data)

X_xl = data['headline'].values
y_xl = data['clickbait'].values

X_train_xl, X_trainVal_xl, y_train_xl, y_trainVal_xl = train_test_split(X_xl, y_xl, test_size=0.2, random_state=0, stratify=y_xl)
X_test_xl, X_testVal_xl, y_test_xl, y_testVal_xl = train_test_split(X_trainVal_xl, y_trainVal_xl, test_size=0.5, random_state=0, stratify=y_trainVal_xl)

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')

def tokenize(data):
    input_ids = []
    for x in data:
        tokenized = tokenizer.encode_plus(
            x,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            return_tensors='tf'
        )
        input_ids.append(tokenized['input_ids'])
    return np.concatenate(input_ids, axis=0)

X_train_xl = tokenize(X_train_xl)
X_test_xl = tokenize(X_test_xl)
X_testVal_xl = tokenize(X_testVal_xl)

word_inputs = Input(shape=(128,), name='word_inputs', dtype='int32')
xlnet = TFXLNetModel.from_pretrained('xlnet-base-cased')
xlnet_encodings = xlnet(word_inputs)[0]
doc_encoding = squeeze(xlnet_encodings[:, -1:, :], axis=1)
doc_encoding = Dropout(.1)(doc_encoding)
outputs = Dense(1, activation='sigmoid', name='outputs')(doc_encoding)
xlnetmodel = Model(inputs=[word_inputs], outputs=[outputs])
xlnetmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=2e-5), metrics=['accuracy'])

epochs_xl = 1
history_xl = xlnetmodel.fit(X_train_xl, y_train_xl, validation_data=(X_testVal_xl, y_testVal_xl), epochs=epochs_xl, batch_size=16)

# Save the trained model
xlnetmodel.save('clickbait_model.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# from tensorflow.keras.models import load_model
# from transformers import XLNetTokenizer, TFXLNetModel
# 
# st.title("News Headline Clickbait Detection")
# st.markdown("---")
# 
# headline = st.text_input("Enter the news headline")
# 
# if st.button("Detect"):
#     # Load the trained model with custom objects
#     model = load_model('clickbait_model.h5', custom_objects={'TFXLNetModel': TFXLNetModel})
# 
#     tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
#     input_data = tokenizer.encode_plus(
#         headline,
#         add_special_tokens=True,
#         max_length=128,
#         padding='max_length',
#         return_tensors='tf'
#     )['input_ids']
# 
#     input_data = np.reshape(input_data, (1, 128))  # Reshape input data
# 
#     prediction = model.predict(input_data)
#     prediction_label = "Clickbait" if prediction[0][0] >= 0.5 else "Not Clickbait"
# 
#     st.markdown("### Prediction Result:")
#     st.markdown(f"The input news headline is classified as **{prediction_label}**.")

# !npm install localtunnel

# !streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com

# !npx localtunnel --port 8501